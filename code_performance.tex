\title{Star Formation and Feedback in Stellar Clusters and Galaxies.\\
       Code performance and scaling.}

\documentclass[11pt]{article}

\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{epstopdf}
\epstopdfsetup{update}
\citestyle{aa}

\newcommand {\apj}{ApJ}
\newcommand {\aj}{AJ}
\newcommand {\apjs}{ApJS}
\newcommand {\apjl}{ApJL}
\newcommand {\mnras}{MNRAS}
\newcommand {\aap}{A\&A}
\newcommand {\aapr}{A\&ARv}
\newcommand {\araa}{ARA\&A}
\newcommand {\pasj}{PASJ}
\newcommand {\pasp}{PASP}
\newcommand {\bain}{Bulletin of the Astronomical Institutes of the Netherlands}
\newcommand {\fcp}{Fundamentals of Cosmic Physics}
\newcommand {\nat}{Nature}
\newcommand {\na}{New Astronomy}
\newcommand{\eg}{e.g.,}
\newcommand\rmxaa{Rev. Mex. Astron. Astrofis.} % Revista Mexicana de Astronomia y Astrofisica

\begin{document}
\maketitle

\section{Introduction}



\subsection{\textsc{Enzo}}

{\bf this text is rough... ingore intro text }
Our simulations of star formation and feedback in dwarf galaxies will utilize the well tested and publicly available \textsc{Enzo} \citep{Enzo-method}. \textsc{Enzo} is a community driven astrophysical simulation code with substantial active use and development, used in publications studying a wide variety of astrophysical processes, from isolated star clusters and galaxies to full cosmological simulations. In modifying this code to incorporate new models of star formation, feedback, and chemical evolution, we take advantage of a wealth of previously developed and tested methods included in the latest development version of \textsc{Enzo}. We outline pre-existing methods below, which dominate the performance budget of our simulations. 

\textsc{Enzo} is an adaptive mesh refinement (AMR) hydrodynamics code with cosmological and magnetohydrodynamics capabilities. We use the direct-Eulerian piecewise parabolic method (PPM) \citep{ColellaWoodward1984, Bryan1995} to solve the equations of hydrodynamics on a spatially and temporally adaptive hierarchy of rectangular grids. \textsc{Enzo} allows for multi-level adaptive refinement of regions of interest, with independent timesteps for each level of refinement, minimizing the cost of resolving the large dynamic ranges required to accurately capture star formation and feedback in our galaxies. We couple \textsc{Enzo} to the \textsc{Grackle} chemistry and radiative cooling library to follow the non-equilibrium primordial chemistry network of 9 species with tabulated metal cooling rates from \textsc{Cloudy} (CITE). Additionally, this is coupled with a metagalctic UV background from \cite{HM2012} with an approximate self-shielding methods adapted from \cite{XX}.  

Although a static, background dark matter gravitational potential is the globally dominant source of gravity in our isolated galaxy simulations, the gravitational potential of baryons and stars are locally important for star formation and feedback. We include gas self-gravity using a multigrid fast Fourier method for solving the Poisson equation \citep{HockneyEastwood1988}. The N-body dynamics of star particles, formed in high density regions of gas using methods extended from \cite{Goldbaum}, are followed via a drift-kick-drift algorithm \citep{HickneyEastwood1988} with timesteps set by the highest refined grid occupied by the particle. In order to adequately resolve the feedback physics originating from our star particles, each is considered a ``must refine'' particle, forcing refinement to the highest refinement level in the region around each particle. Initial star particle properties are modeled through interpolation on initial values from a stellar evolution track (REFERENCE), with radiation properties interpolated from the OSTAR2002 (CITE) grid of stellar models. Stars evolve over time through feedback as either stellar winds or in supernova explosions as determined from interpolating on the NuGrid data set of stellar yields (CITE). This feedback in both thermal and kinetic energy is applied to the physical regions around each star following the methods in (Simpson et al). We follow the resulting chemical evolution of our galaxy from ejected stellar yields using a total metallicity tracer field, along with tracer fields for (number) of individually selected elemental species. At formation, star particles additionally contain chemical tags of the metallicity and elemental abundances of the gas region where they formed.

We use the direct ray tracing methods included in \textsc{Enzo} to track the ionizing radiation from massive stars in two energy bins, which is tied to the chemistry solver and is coupled to the hydrodynamics through radiation pressure feedback. We plan to additionally include cosmic rays in our simulation, a non-thermal source of feedback generated in shocks from supernova explosions, using the diffusive, two-fluid model of \cite{Salem}. This model was initially implemented in \textsc{ENZO}'s ZEUS hydrodynamics solver and is currently being adapted to the PPM solver used in this work. 

\section{Performance and Scaling of Base Code}

Understanding the exact scaling and performance properties of an AMR simulation is challenging, as the total number of computational grids can vary dramatically over the course of the simulation run. In the case of our dwarf galaxy simulations, the number of grids will initially be relatively insignificant, but will grow substantially as the gas collapses into dense regions and forms stars. Based on our test simulations, we expect the number of grid cells to be roughly consistent from this point onward, fluctuating around some mean value. From this point forward, code performance (number of cell updates per wall second) will be roughly consistent, with computational expense (SU per simulation Myr) determined by the size of each timestep. The timestep size is determined using the CFL (explain) condition, and will be sensitive to whether or not stellar winds and supernovae (which produce hot, fast gas) exist on the grid. However, radiative transfer will impact performance, correlating with both the number of ionizing sources (of which we expect at most a few hundred at any one time) and their physical concentration, which limits how evenly they can be distributed among processors, reducing the effectiveness of load balancing. In the below we outline the results of our scaling tests to show how performance and expense scales with computational load and the number of radiation sources. 

\subsection{Code Performance and Grid Hierarchy}

We begin by illustrating how our computational domain is divided between individual levels in an AMR run of our dwarf galaxy. Figure~\ref{fig:levels} shows the results of two test simulations of our dwarf galaxy in the planned production scale box ($16.384^3$ kpc with a $128^3$ root grid) at a maximum refinement level of 6 and 7, corresponding to a physical resolution of 2.0 and 1.0 pc respectively. As demonstrated, the amount of wall time spent on a single cycle is dominated by computation on the highest level of refinement (top left panel). This is because the number of cells on a given level generally increases towards higher levels of refinement (top right panel), with the most cells on a given level at the highest refinement level, and because the time step is allowed to vary on each level, scaling linearly with the cell size. In reality, the typical timestep scales much more steeply with refinement level as the feedback physics (with hot gas and high velocities) is injected at the maximum resolution, and we expect most of the cells in the root grid to be sufficiently far from the galaxy to be devoid of any hot, fast moving gas. 

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{notebooks/levels}
\caption{Break down of grid hierarchy and performance on each level for a two test simulations of our fiducial model at two maximum refinement levels, level 6 (dashed) and level 7 (solid), corresponding to physical resolutions of 2.0 and 1.0 pc respectively. Each was run on Stampede with 128 processors. Shown is the wall time spent on each level, normalized by the root grid time (top left), the fraction of all cells on each level (top right), the effeciency of each level relative to the root grid, defined as the number of cell updates per second per processor (bottom left), and an estimate for the the theoretical minimum SU cost per Gyr of simulation time. See text for further discussion of each of these points.}
\label{fig:levels}
\end{figure}

The lower left panel of Figure\ref{fig:levels} provides the relative efficiency of each level, as normalized to the efficiency of level 0. We define efficiency as the number of cell updates per second per processor, with typical values of 10$^4$ as averaged over all levels. The efficiency is determined primarily by the ability of \textsc{Enzo} to properly load balance across all processors, which becomes inefficient if there are too few cells on a given level. For example, levels 1 and 2 have low cell counts and low efficiencies; however, low efficiencies for this reason are generally irrelevant, as very little time is actually spent updating these cells. The efficiency at he highest levels of refinement is generally around $2\times 10^4$ cells/s/proc. Assuming all levels can operate completely independently (which is physically impossible), we present the theoretical lower limit cost of 1 Gyr of simulation time per level on 128 cores in the lower right panel; the horizontal line gives the sum over all levels. To compute this, we adopt typical time steps for each level as obtained from our test simulations using only supernovae feedback and stellar winds. This means a typical time step of roughly 300 years at level 7, and 600 years at level 6. In practice, the levels cannot operate independently; a more realistic estimate is a factor of 2-3 larger than this theoretical minimum.

\begin{figure}
\centering
\includegraphics[width=0.45\linewidth]{notebooks/radiation}
\caption{Estimate of relative slow down of a test simulation as a function of the number of ionizing sources. The test simulation is of our dwarf galaxy at 8.0 pc resolution, or a maximum refinement level of 4, run on 32 processors on Stampede. This test simulation included supernovae, stellar winds, photoelectric heating, and radiative transfer. We estimate that there will be at most a few hundred sources at any one moment in our simulation, implying a maximal slow down of about a factor of 10.}
\label{fig:radiation}
\end{figure}

We demonstrate the effects of including radiative transfer as a function of the number of ionizing particles in Figure~\ref{fig:radiation} for a test simulation run with a maximum refinement level of 4 (8.0 pc resolution) on 64 processors. Shown is the slow down of a given cycle as a function of the number of ionizing sources. We compute this as the number of cell updates per second per particle, normalized to the zero particle case. This implicitly assumes that any decrease in performance is due solely to the number of sources, and no other factors. To be clear, the additional computational time incurred by including radiation is split in into three parts, 1) evolving the photon packages through the grid cells, 2) ray tracing from each source to deposit the photon packages, and 3) communication time communicating photon packages between processors. For this reason, the additional cost also depends upon the nature of the sources themselves, their clustering, and the details of the medium the photons propagate through.

\subsection{Scaling}

\begin{figure}
\centering
\includegraphics[width=0.45\linewidth]{notebooks/scaling}
\caption{A demonstration of weak scaling for our two test runs with maximum refinement levels of 6 (dashed) and 7 (solid). Scaling works very well for up to 128 cores, but becomes less than ideal about this amount. We anticipate to run with either 128 or 256 cores for all of our simulations depending on how the grid and particle count evolve in the simulation.}
\label{fig:scaling}
\end{figure}

\bibliographystyle{apj}
\bibliography{msbib}


\end{document}
